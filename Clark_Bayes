# -*- coding: utf-8 -*-
"""
Created on Tue Jul 11 09:24:05 2017

@author: dahoang
"""

#import math
import numpy as np
import pymc3 as pm
#import theano
#import theano.tensor as tt

#from datetime import datetime

print 'start'
#things to be user-input, for now dummies
Premiums = [5812.]#,4908,5454]
obs = [1722.,3830.,3603.,3835.,3873.,3895.,3918.,3917.]
n_iter = 1000
gamma_k = 0.5

# define some growth functions
def loglogistic(x, omega, theta):
    return (x**(omega)) / ((x**(omega)) + (theta**(omega)))
# using a Weibull function
def weibull(x, omega, theta):
    return 1. - np.exp(-1 * ((x / theta)**omega))
    

with pm.Model() as Clark_Bayes:
    # Assign the hyperpriors
    log_gamma_omega_theta = pm.Normal('log_gamma_omega_theta', 0, 100**2, shape=3)
    sig_gamyr = pm.Uniform('sig_gamyr', 0, 100, shape=1)
    # the paper specifies the Inverse-Wishart for sampling the covariance matrix, 
    # but pymc3 documentation prohibits this use for now
    # instead, LKJCholeskyCov or LKJCorr is recommended, but these are problematic too
    #Sigma = pm.LKJCholeskyCov('Sigma', eta=10000, n=3, sd_dist=None)
    # wasn't sure how to assign eta, for now a dummy...but as eta->Inf, LKJCorr goes to identity matrix
    # for now hard code as identity matrix
    Sigma = np.eye(3)
    
    # company-level 
    log_allks = pm.MvNormal('log_allks', mu=log_gamma_omega_theta, cov=Sigma, shape=3)
    
    # now start describing each company's data in individual accident years
    log_gamma_ik = pm.Normal('log_gamma_ik', log_allks[0], sig_gamyr**2, shape=1, observed=pm.math.log(obs[len(obs)-1]/Premiums[0]), testval=0.1)#obs[len(obs)-1]/Premiums[0])#testval=0, 
    #log_gamma_ik = pm.Normal('log_gamma_ik', log_gamma, sig_gamyr**2, shape=1, observed=)#, testval=0)
    # for now, the shape is 1, because the model is written for an individual AY
    
    # some more priors at the company-level
    rho = pm.Uniform('rho', -1,1, shape=1, testval=0.9)
    sig_k = pm.Uniform('sig_k', 0, 100, shape=1, testval=0.1)
    
    # finally, let's construct noise of payments in this AY
    #rho2 = np.zeros(len(obs)) + rho
    #sig2 = np.zeros(len(obs)) + sig_k
    #print sig2
    #print sig2.eval()
    del_ik = pm.Normal('del_ik', 0, (sig_k**2)*(1-(rho**2)), shape=len(obs)) #matches to shape 1!!!
    #del_ik = pm.Normal('del_ik', 0, (sig2**2)*(1-(rho2**2)), shape=len(obs))
#    #eps_ik_t0 = pm.Normal('eps_ik_t0', 0, sig_k**2, shape=len(obs))
#    #AR1 which takes k: the lag effect and tau: the precision.
#    #this passes without crashing! just make sure tau is set right
#    s = pm.AR1('s', k=rho, tau_e= 1/((sig_k**2)*(1-(rho**2))), shape=len(obs))
#    eps_ik = pm.Deterministic('eps_ik', s+del_ik)#, shape=len(obs))
    
#    # now construct the payments themselves, for now just for an AY row
#    # G is a vector representing the growth function
#    G = [loglogistic(i, pm.math.exp(log_allks[1]), pm.math.exp(log_allks[2])) for i in range(len(obs))]
#    p = [Premiums[0] for i in range(len(obs))]
#    mu_ik = np.array([(p[i]*eps_ik[i]*G[i]) + del_ik[i] for i in range(len(obs))])
#    #mu_ik_res = pm.Deterministic('mu_ik_res', mu_ik*1)#, shape=len(obs))
#    log_y_ik = pm.Deterministic('y_ik', pm.math.log(mu_ik) + eps_ik)#, observed=pm.math.log(obs))#, shape=len(obs))
#    # would like to incorporate observed at log_y_ik, but Deterministic can't take observed data
#    # might need to use Potential
#    # https://github.com/pymc-devs/pymc3/issues/1971
#    # https://stats.stackexchange.com/questions/251280/what-is-pm-potential-in-pymc3
    

print 'just did the model, now sample...'
#with Clark_Bayes:
#    try:
#        print 'sampling using MAP'
#        start = pm.find_MAP()
#        # NUTS will always take longer, but samples more efficiently (w.r.t. effective sample size)
#        # ...paper uses Metropolis-within-Gibbs, but not availible in pymc3 for continuous dists
#        step = pm.Metropolis()
#        trace = pm.sample(n_iter, start=start, step=step)#
#        print start
#        print 'done sampling using MAP'
#    except:
#        step = pm.Metropolis()
#        trace = pm.sample(n_iter, step=step)
#        print 'done sampling without MAP'

with Clark_Bayes:
    print 'sampling using MAP'
    #start = pm.find_MAP()
    #print start
    # NUTS will always take longer, but samples more efficiently (w.r.t. effective sample size)
    # ...paper uses Metropolis-within-Gibbs, but not availible in pymc3 for continuous dists
    #step = pm.Metropolis()
    trace = pm.sample(n_iter, n_init=200000)#, step=step)#)#, start=start)#
    #print start
    print 'done sampling using MAP'

try:
    pm.traceplot(trace)
except:
    print 'probably dont have enough samples to make good traceplots...'

# other references:
# https://github.com/pymc-devs/pymc3/issues/884
# https://github.com/pymc-devs/pymc3/issues/555
# https://github.com/pymc-devs/pymc3/issues/1163
# https://github.com/pymc-devs/pymc3/issues/535
# https://stackoverflow.com/questions/41909711/pymc3-nuts-sampling-multiple-hierarchical-model-wishart-distribution
# https://stackoverflow.com/questions/33661064/pymc3-hierarchical-model-with-multiple-obsesrved-variables
# https://am207.github.io/2017/wiki/Lab7_bioassay.html#convergence-diagnostics


# pickle output
